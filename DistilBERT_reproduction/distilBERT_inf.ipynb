import time
import os
import torch
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# Config
TASK = "stsb"       
BATCH_SIZE = 32
MAX_LENGTH = 128
WARMUP_STEPS = 10
NUM_THREADS = 4
NUM_INTEROP_THREADS = 1

MODELS = {
    "BERT-base": "bert-base-uncased",
    "DistilBERT": "distilbert-base-uncased",
}

def count_params_m(model):
    return sum(p.numel() for p in model.parameters()) / 1e6

def make_batches(tok_ds, batch_size):
    """Prebuild batches so DataLoader/Python overhead isn't in the timed loop."""
    batches = []
    n = len(tok_ds)
    for i in range(0, n, batch_size):
        chunk = tok_ds[i:i + batch_size]
        batch = {
            "input_ids": torch.tensor(chunk["input_ids"], dtype=torch.long),
            "attention_mask": torch.tensor(chunk["attention_mask"], dtype=torch.long),
        }
        # Some models may include token_type_ids (BERT) and some won't (DistilBERT)
        if "token_type_ids" in chunk:
            batch["token_type_ids"] = torch.tensor(chunk["token_type_ids"], dtype=torch.long)
        batches.append(batch)
    return batches

def benchmark_full_pass(model, batches):
    model.eval()

    # Warmup
    with torch.inference_mode():
        for b in batches[:WARMUP_STEPS]:
            _ = model(**b)

    # Timed pass
    start = time.perf_counter()
    with torch.inference_mode():
        for b in batches:
            _ = model(**b)
    end = time.perf_counter()
    return end - start

def main():
    # Threading control
    if NUM_THREADS is not None:
        torch.set_num_threads(NUM_THREADS)
    if NUM_INTEROP_THREADS is not None:
        torch.set_num_interop_threads(NUM_INTEROP_THREADS)

    os.environ["TOKENIZERS_PARALLELISM"] = "false"

    ds = load_dataset("glue", TASK, split="validation")

    def to_text(batch):
        return {"text_a": batch["sentence1"], "text_b": batch["sentence2"]}

    ds = ds.map(to_text, remove_columns=[c for c in ds.column_names if c not in ["text_a", "text_b"]])

    results = []

    for name, model_id in MODELS.items():
        tokenizer = AutoTokenizer.from_pretrained(model_id)

        def tok(batch):
            return tokenizer(
                batch["text_a"],
                batch["text_b"],
                truncation=True,
                max_length=MAX_LENGTH,
                padding="max_length",  
            )

        tok_ds = ds.map(tok, batched=True, remove_columns=["text_a", "text_b"])

        # Prebuild batches once
        batches = make_batches(tok_ds, BATCH_SIZE)

        model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=1)
        model.to("cpu")

        params_m = count_params_m(model)
        secs = benchmark_full_pass(model, batches)

        results.append((name, params_m, secs))

    print(f"\nTable (CPU, batch_size={BATCH_SIZE}, fixed_len={MAX_LENGTH}, full pass of GLUE STS-B validation)\n")
    print(f"{'Model':<12} {'# param (M)':>12} {'Inf. time (s)':>15}")
    for name, pm, s in results:
        print(f"{name:<12} {pm:>12.1f} {s:>15.2f}")

    bert_time = next(s for n, _, s in results if n == "BERT-base")
    dist_time = next(s for n, _, s in results if n == "DistilBERT")
    print(f"\nSpeedup (BERT / DistilBERT): {bert_time / dist_time:.2f}x faster")

if __name__ == "__main__":
    main()
